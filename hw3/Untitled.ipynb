{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In amazon_cells_labelled : 500 bad comments 500 good comments\n",
      "In imdb_labelled : 500 bad comments 500 good comments\n",
      "In yelp_labelled : 500 bad comments 500 good comments\n"
     ]
    }
   ],
   "source": [
    "files=['amazon_cells_labelled','imdb_labelled','yelp_labelled']\n",
    "dataset={} #structure {'amazon': [[bad comments],[good comments]], ...}\n",
    "for file in files:\n",
    "    dataset[file]=[[] for i in range(2)]\n",
    "    f = open(file+'.txt')\n",
    "    for line in f:\n",
    "        comment, label=line.split('\\t')\n",
    "        dataset[file][int(label)].append(comment)\n",
    "    \n",
    "for key, value in dataset.items():\n",
    "    print('In',key,':',len(value[0]),'bad comments',len(value[1]),'good comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "stemmer=nltk.SnowballStemmer(\"english\")\n",
    "import re\n",
    "for key, value in dataset.items():\n",
    "    for comments in value:\n",
    "        for i in range(len(comments)):\n",
    "            lowered=comments[i].lower()\n",
    "            word_list=re.findall(r\"[a-z]+\", lowered)\n",
    "            filtered_words = [stemmer.stem(word) for word in word_list if word not in stopwords]\n",
    "            comments[i] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, trainLabel, testData, testLabel=[], [], [], []\n",
    "for key, value in dataset.items():\n",
    "    for i in range(2):\n",
    "        trainData+=value[i][:400]\n",
    "        trainLabel+=[i]*400\n",
    "        testData+=value[i][400:]\n",
    "        testLabel+=[i]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words=set()\n",
    "for comment in trainData: words.update(set(comment))\n",
    "words=list(words)\n",
    "BoWTrainData=[]\n",
    "for comment in trainData: \n",
    "    bag =[0 for _ in range(len(words))]\n",
    "    for word in comment: \n",
    "        if word in words: bag[words.index(word)]+=1\n",
    "    BoWTrainData.append(bag)\n",
    "BoWTestData=[]\n",
    "for comment in testData: \n",
    "    bag =[0 for _ in range(len(words))]\n",
    "    for word in comment: \n",
    "        if word in words: bag[words.index(word)]+=1\n",
    "    BoWTestData.append(bag)\n",
    "# for i in range(2): print(bagsOfWords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "logNormedBoWTrainData=np.log(np.array(BoWTrainData)+1)\n",
    "logNormedBoWTestData=np.log(np.array(BoWTestData)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 3373)\n",
      "Using Logistic Regression:\n",
      "0.821666666667\n",
      "[[265  35]\n",
      " [ 72 228]]\n",
      "Using Gaussian Naive Bayes:\n",
      "0.638333333333\n",
      "[[267  33]\n",
      " [184 116]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "logistic = LogisticRegression().fit(logNormedBoWTrainData, trainLabel)\n",
    "gnb = GaussianNB().fit(logNormedBoWTrainData, trainLabel)\n",
    "logistic_pred = logistic.predict(logNormedBoWTestData)\n",
    "gnb_pred = gnb.predict(logNormedBoWTestData)\n",
    "print('Using Logistic Regression:')\n",
    "print(accuracy_score(testLabel, logistic_pred))\n",
    "print(confusion_matrix(testLabel,logistic_pred))\n",
    "print('Using Gaussian Naive Bayes:')\n",
    "print(accuracy_score(testLabel, gnb_pred))\n",
    "print(confusion_matrix(testLabel,gnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
