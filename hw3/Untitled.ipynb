{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In amazon_cells_labelled : 500 bad comments 500 good comments\n",
      "In imdb_labelled : 500 bad comments 500 good comments\n",
      "In yelp_labelled : 500 bad comments 500 good comments\n"
     ]
    }
   ],
   "source": [
    "files=['amazon_cells_labelled','imdb_labelled','yelp_labelled']\n",
    "dataset={} #structure {'amazon': [[bad comments],[good comments]], ...}\n",
    "for file in files:\n",
    "    dataset[file]=[[] for i in range(2)]\n",
    "    f = open(file+'.txt')\n",
    "    for line in f:\n",
    "        comment, label=line.split('\\t')\n",
    "        dataset[file][int(label)].append(comment)\n",
    "    \n",
    "for key, value in dataset.items():\n",
    "    print('In',key,':',len(value[0]),'bad comments',len(value[1]),'good comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "stemmer=nltk.SnowballStemmer(\"english\")\n",
    "import re\n",
    "for key, value in dataset.items():\n",
    "    for comments in value:\n",
    "        for i in range(len(comments)):\n",
    "            lowered=comments[i].lower()\n",
    "            word_list=re.findall(r\"[a-z]+\", lowered)\n",
    "            filtered_words = [stemmer.stem(word) for word in word_list if word not in stopwords]\n",
    "            comments[i] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData, trainLabel, testData, testLabel=[], [], [], []\n",
    "for key, value in dataset.items():\n",
    "    for i in range(2):\n",
    "        trainData+=value[i][:400]\n",
    "        trainLabel+=[i]*400\n",
    "        testData+=value[i][400:]\n",
    "        testLabel+=[i]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words=set()\n",
    "for comment in trainData: words.update(set(comment))\n",
    "words=list(words)\n",
    "BoWTrainData=[]\n",
    "for comment in trainData: \n",
    "    bag =[0 for _ in range(len(words))]\n",
    "    for word in comment: \n",
    "        if word in words: bag[words.index(word)]+=1\n",
    "    BoWTrainData.append(bag)\n",
    "BoWTestData=[]\n",
    "for comment in testData: \n",
    "    bag =[0 for _ in range(len(words))]\n",
    "    for word in comment: \n",
    "        if word in words: bag[words.index(word)]+=1\n",
    "    BoWTestData.append(bag)\n",
    "# for i in range(2): print(BoWTrainData[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #   We choose to do log-normalization.\n",
    "    #   In the context of product review, the label of whether the review of the product is good or bad\n",
    "    #   should align with human intuition, that is, the more positive and optimistic words in the review\n",
    "    #   the more positive the review it is. However, a more confusing review will be comprehensive one,\n",
    "    #   where people write both positive and negative comments all in one. In this case, we the best way\n",
    "    #   we can tell the sentiment is to check the ratio of positive comment to negative comment. If we log\n",
    "    #   them, we are making the difference of the two parts less noticeable if the comments are significantly\n",
    "    #   long in terms of total word counts, and this is what we need, and it decreases the variance.\n",
    "    #   And the reason of not using the others is the following:\n",
    "    #   l1-norm is adjusting the weight of each word with respect to the length of comment linearly, which also make the difference of the two parts less noticeable\n",
    "    #   however, it does not have the log property which make the difference addressable when comments are short\n",
    "    #   but not when it is significantly long. l2-norm did it in an exponential way, which is not satisfactory\n",
    "    #   when comment length vary when the length is small, say less than 5 critical words.\n",
    "    #   The last approach of standardizing the data by subtracting the mean and dividing by the variance\n",
    "    #   will do the exactly the reverse which increase the variance because our data are either positive\n",
    "    #   or zero, which will gives a variance in between 0 and 1 which finally increase the relative\n",
    "    #   distance of the data drastically if data are divided by the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "logNormedBoWTrainData=np.log(np.array(BoWTrainData)+1)\n",
    "logNormedBoWTestData=np.log(np.array(BoWTestData)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Logistic Regression:\n",
      "0.821666666667\n",
      "[[265  35]\n",
      " [ 72 228]]\n",
      "Using Gaussian Naive Bayes:\n",
      "0.638333333333\n",
      "[[267  33]\n",
      " [184 116]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "logistic = LogisticRegression().fit(logNormedBoWTrainData, trainLabel)\n",
    "gnb = GaussianNB().fit(logNormedBoWTrainData, trainLabel)\n",
    "logistic_pred = logistic.predict(logNormedBoWTestData)\n",
    "gnb_pred = gnb.predict(logNormedBoWTestData)\n",
    "print('Using Logistic Regression:')\n",
    "print(accuracy_score(testLabel, logistic_pred))\n",
    "print(confusion_matrix(testLabel,logistic_pred))\n",
    "print('Using Gaussian Naive Bayes:')\n",
    "print(accuracy_score(testLabel, gnb_pred))\n",
    "print(confusion_matrix(testLabel,gnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def NgramDataTransformation(n, row):\n",
    "    ret = []\n",
    "    for i in range(len(row) - n):\n",
    "        stringToAdd = \"\"\n",
    "        for j in range(n):\n",
    "            stringToAdd += row[i+j]\n",
    "        ret.append(stringToAdd)\n",
    "    return ret\n",
    "\n",
    "newTrainingData = [NgramDataTransformation(2, row) for row in trainData]\n",
    "newTestData = [NgramDataTransformation(2, row) for row in testData]\n",
    "\n",
    "words=set()\n",
    "for comment in newTrainingData: words.update(set(comment))\n",
    "words=list(words)\n",
    "BoWnewTrainData=[]\n",
    "for comment in newTrainingData: \n",
    "    bag =[0 for _ in range(len(words))]\n",
    "    for word in comment: \n",
    "        if word in words: bag[words.index(word)]+=1\n",
    "    BoWnewTrainData.append(bag)\n",
    "BoWnewTestData=[]\n",
    "for comment in newTestData: \n",
    "    bag =[0 for _ in range(len(words))]\n",
    "    for word in comment: \n",
    "        if word in words: bag[words.index(word)]+=1\n",
    "    BoWnewTestData.append(bag)\n",
    "# for i in range(2): print(BoWnewTrainData[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "logNormedBoWnewTrainData=np.log(np.array(BoWnewTrainData)+1)\n",
    "logNormedBoWnewTestData=np.log(np.array(BoWnewTestData)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Logistic Regression:\n",
      "0.571666666667\n",
      "[[271  29]\n",
      " [228  72]]\n",
      "Using Gaussian Naive Bayes:\n",
      "0.576666666667\n",
      "[[283  17]\n",
      " [237  63]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "logistic = LogisticRegression().fit(logNormedBoWnewTrainData, trainLabel)\n",
    "gnb = GaussianNB().fit(logNormedBoWnewTrainData, trainLabel)\n",
    "logistic_pred = logistic.predict(logNormedBoWnewTestData)\n",
    "gnb_pred = gnb.predict(logNormedBoWnewTestData)\n",
    "print('Using Logistic Regression:')\n",
    "print(accuracy_score(testLabel, logistic_pred))\n",
    "print(confusion_matrix(testLabel,logistic_pred))\n",
    "print('Using Gaussian Naive Bayes:')\n",
    "print(accuracy_score(testLabel, gnb_pred))\n",
    "print(confusion_matrix(testLabel,gnb_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
