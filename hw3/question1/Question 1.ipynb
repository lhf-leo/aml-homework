{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In amazon_cells_labelled : 500 bad comments 500 good comments\n",
      "In imdb_labelled : 500 bad comments 500 good comments\n",
      "In yelp_labelled : 500 bad comments 500 good comments\n"
     ]
    }
   ],
   "source": [
    "files=['amazon_cells_labelled','imdb_labelled','yelp_labelled']\n",
    "dataset={} #structure {'amazon': [[bad comments],[good comments]], ...}\n",
    "for file in files:\n",
    "    dataset[file]=[[] for i in range(2)]\n",
    "    f = open(file+'.txt')\n",
    "    for line in f:\n",
    "        comment, label=line.split('\\t')\n",
    "        dataset[file][int(label)].append(comment)\n",
    "    \n",
    "for key, value in dataset.items():\n",
    "    print('In',key,':',len(value[0]),'bad comments',len(value[1]),'good comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "stemmer=nltk.SnowballStemmer(\"english\")\n",
    "import re\n",
    "for key, value in dataset.items():\n",
    "    for comments in value:\n",
    "        for i in range(len(comments)):\n",
    "            lowered=comments[i].lower()\n",
    "            word_list=re.findall(r\"[a-z]+\", lowered)\n",
    "            filtered_words = [stemmer.stem(word) for word in word_list if word not in stopwords]\n",
    "            comments[i] = filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData, trainLabel, testData, testLabel=[], [], [], []\n",
    "for key, value in dataset.items():\n",
    "    for i in range(2):\n",
    "        trainData+=value[i][:400]\n",
    "        trainLabel+=[i]*400\n",
    "        testData+=value[i][400:]\n",
    "        testLabel+=[i]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words=set()\n",
    "for comment in trainData: words.update(set(comment))\n",
    "words=list(words)\n",
    "BoWTrainData=[]\n",
    "for comment in trainData: \n",
    "    bag =[0 for _ in range(len(words))]\n",
    "    for word in comment: \n",
    "        if word in words: bag[words.index(word)]+=1\n",
    "    BoWTrainData.append(bag)\n",
    "BoWTestData=[]\n",
    "for comment in testData: \n",
    "    bag =[0 for _ in range(len(words))]\n",
    "    for word in comment: \n",
    "        if word in words: bag[words.index(word)]+=1\n",
    "    BoWTestData.append(bag)\n",
    "# for i in range(2): print(BoWTrainData[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "  We choose to do log-normalization.\n",
    "  In the context of product review, the label of whether the review of the product is good or bad\n",
    "  should align with human intuition, that is, the more positive and optimistic words in the review\n",
    "  the more positive the review it is. However, a more confusing review will be comprehensive one,\n",
    "  where people write both positive and negative comments all in one. In this case, we the best way\n",
    "  we can tell the sentiment is to check the ratio of positive comment to negative comment. If we log\n",
    "  them, we are making the difference of the two parts less noticeable if the comments are significantly\n",
    "  long in terms of total word counts, and this is what we need, and it decreases the variance.\n",
    "  And the reason of not using the others is the following:\n",
    "  l1-norm is adjusting the weight of each word with respect to the length of comment linearly, which also make the difference of the two parts less noticeable\n",
    "  however, it does not have the log property which make the difference addressable when comments are short\n",
    "  but not when it is significantly long. l2-norm did it in an exponential way, which is not satisfactory\n",
    "  when comment length vary when the length is small, say less than 5 critical words.\n",
    "  The last approach of standardizing the data by subtracting the mean and dividing by the variance\n",
    "  will do the exactly the reverse which increase the variance because our data are either positive\n",
    "  or zero, which will gives a variance in between 0 and 1 which finally increase the relative\n",
    "  distance of the data drastically if data are divided by the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "logNormedBoWTrainData=np.log(np.array(BoWTrainData)+1)\n",
    "logNormedBoWTestData=np.log(np.array(BoWTestData)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "poor\n",
      "worst\n",
      "terribl\n",
      "wast\n",
      "slow\n",
      "suck\n",
      "aw\n",
      "disappoint\n",
      "stupid\n",
      "Using Logistic Regression:\n",
      "0.821666666667\n",
      "[[265  35]\n",
      " [ 72 228]]\n",
      "Using Gaussian Naive Bayes:\n",
      "0.638333333333\n",
      "[[267  33]\n",
      " [184 116]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "logistic = LogisticRegression().fit(logNormedBoWTrainData, trainLabel)\n",
    "gnb = GaussianNB().fit(logNormedBoWTrainData, trainLabel)\n",
    "\n",
    "weight_vector = logistic.coef_\n",
    "sorted_weight = [indexweight[0] for indexweight in sorted(enumerate(weight_vector[0]), key = lambda x: x[1])]\n",
    "for i in range(10):\n",
    "    print(words[sorted_weight[i]])\n",
    "# TODO PRINT \"THE TOP 10 MOST IMPORTANT\" WORD STEM?\n",
    "\n",
    "logistic_pred = logistic.predict(logNormedBoWTestData)\n",
    "gnb_pred = gnb.predict(logNormedBoWTestData)\n",
    "print('Using Logistic Regression:')\n",
    "print(accuracy_score(testLabel, logistic_pred))\n",
    "print(confusion_matrix(testLabel,logistic_pred))\n",
    "print('Using Gaussian Naive Bayes:')\n",
    "print(accuracy_score(testLabel, gnb_pred))\n",
    "print(confusion_matrix(testLabel,gnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def NgramDataTransformation(n, row):\n",
    "    ret = []\n",
    "    for i in range(len(row) - n):\n",
    "        stringToAdd = \"\"\n",
    "        for j in range(n):\n",
    "            stringToAdd += row[i+j]\n",
    "        ret.append(stringToAdd)\n",
    "    return ret\n",
    "\n",
    "newTrainingData = [NgramDataTransformation(2, row) for row in trainData]\n",
    "newTestData = [NgramDataTransformation(2, row) for row in testData]\n",
    "\n",
    "words=set()\n",
    "for comment in newTrainingData: words.update(set(comment))\n",
    "words=list(words)\n",
    "BoWnewTrainData=[]\n",
    "for comment in newTrainingData: \n",
    "    bag =[0 for _ in range(len(words))]\n",
    "    for word in comment: \n",
    "        if word in words: bag[words.index(word)]+=1\n",
    "    BoWnewTrainData.append(bag)\n",
    "BoWnewTestData=[]\n",
    "for comment in newTestData: \n",
    "    bag =[0 for _ in range(len(words))]\n",
    "    for word in comment: \n",
    "        if word in words: bag[words.index(word)]+=1\n",
    "    BoWnewTestData.append(bag)\n",
    "# for i in range(2): print(BoWnewTrainData[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "logNormedBoWnewTrainData=np.log(np.array(BoWnewTrainData)+1)\n",
    "logNormedBoWnewTestData=np.log(np.array(BoWnewTestData)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Logistic Regression:\n",
      "0.571666666667\n",
      "[[271  29]\n",
      " [228  72]]\n",
      "Using Gaussian Naive Bayes:\n",
      "0.576666666667\n",
      "[[283  17]\n",
      " [237  63]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "\n",
    "logistic = LogisticRegression().fit(logNormedBoWnewTrainData, trainLabel)\n",
    "gnb = GaussianNB().fit(logNormedBoWnewTrainData, trainLabel)\n",
    "logistic_pred = logistic.predict(logNormedBoWnewTestData)\n",
    "gnb_pred = gnb.predict(logNormedBoWnewTestData)\n",
    "print('Using Logistic Regression:')\n",
    "print(accuracy_score(testLabel, logistic_pred))\n",
    "print(confusion_matrix(testLabel,logistic_pred))\n",
    "print('Using Gaussian Naive Bayes:')\n",
    "print(accuracy_score(testLabel, gnb_pred))\n",
    "print(confusion_matrix(testLabel,gnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean=logNormedBoWTrainData.mean(0)\n",
    "_,_,vTrain=np.linalg.svd(logNormedBoWTrainData - mean[np.newaxis, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q = 10:\n",
      "Using Logistic Regression:\n",
      "0.61\n",
      "[[248  52]\n",
      " [182 118]]\n",
      "Using Gaussian Naive Bayes:\n",
      "0.586666666667\n",
      "[[238  62]\n",
      " [186 114]]\n",
      "q = 50:\n",
      "Using Logistic Regression:\n",
      "0.696666666667\n",
      "[[256  44]\n",
      " [138 162]]\n",
      "Using Gaussian Naive Bayes:\n",
      "0.64\n",
      "[[239  61]\n",
      " [155 145]]\n",
      "q = 100:\n",
      "Using Logistic Regression:\n",
      "0.708333333333\n",
      "[[240  60]\n",
      " [115 185]]\n",
      "Using Gaussian Naive Bayes:\n",
      "0.66\n",
      "[[251  49]\n",
      " [155 145]]\n"
     ]
    }
   ],
   "source": [
    "def pca_implementation(q):\n",
    "    matrix_w=vTrain[:q, :]\n",
    "\n",
    "    reducedTrain = (logNormedBoWTrainData - mean[np.newaxis, :]).dot(matrix_w.T)\n",
    "    reducedTest = (logNormedBoWTestData - mean[np.newaxis, :]).dot(matrix_w.T)\n",
    "\n",
    "    logistic = LogisticRegression().fit(reducedTrain, trainLabel)\n",
    "    gnb = GaussianNB().fit(reducedTrain, trainLabel)\n",
    "    logistic_pred = logistic.predict(reducedTest)\n",
    "    gnb_pred = gnb.predict(reducedTest)\n",
    "    print('Using Logistic Regression:')\n",
    "    print(accuracy_score(testLabel, logistic_pred))\n",
    "    print(confusion_matrix(testLabel,logistic_pred))\n",
    "    print('Using Gaussian Naive Bayes:')\n",
    "    print(accuracy_score(testLabel, gnb_pred))\n",
    "    print(confusion_matrix(testLabel,gnb_pred))\n",
    "    \n",
    "print(\"q = 10:\")\n",
    "pca_implementation(10)\n",
    "print(\"q = 50:\")\n",
    "pca_implementation(50)\n",
    "print(\"q = 100:\")\n",
    "pca_implementation(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q = 10:\n",
      "Using Logistic Regression:\n",
      "0.61\n",
      "[[248  52]\n",
      " [182 118]]\n",
      "Using Gaussian Naive Bayes:\n",
      "0.586666666667\n",
      "[[238  62]\n",
      " [186 114]]\n",
      "q = 50:\n",
      "Using Logistic Regression:\n",
      "0.696666666667\n",
      "[[256  44]\n",
      " [138 162]]\n",
      "Using Gaussian Naive Bayes:\n",
      "0.64\n",
      "[[239  61]\n",
      " [155 145]]\n",
      "q = 100:\n",
      "Using Logistic Regression:\n",
      "0.708333333333\n",
      "[[240  60]\n",
      " [115 185]]\n",
      "Using Gaussian Naive Bayes:\n",
      "0.66\n",
      "[[251  49]\n",
      " [155 145]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def pca_library(q):\n",
    "    pca = PCA(n_components=q,svd_solver='full').fit(logNormedBoWTrainData)\n",
    "    reducedTrain=pca.transform(logNormedBoWTrainData)\n",
    "    # reducedTest=pca.transform(logNormedBoWTestData)\n",
    "    reducedTest=(logNormedBoWTestData - pca.mean_[np.newaxis, :]).dot(pca.components_.T)\n",
    "\n",
    "    logistic = LogisticRegression().fit(reducedTrain, trainLabel)\n",
    "    gnb = GaussianNB().fit(reducedTrain, trainLabel)\n",
    "    logistic_pred = logistic.predict(reducedTest)\n",
    "    gnb_pred = gnb.predict(reducedTest)\n",
    "    print('Using Logistic Regression:')\n",
    "    print(accuracy_score(testLabel, logistic_pred))\n",
    "    print(confusion_matrix(testLabel,logistic_pred))\n",
    "    print('Using Gaussian Naive Bayes:')\n",
    "    print(accuracy_score(testLabel, gnb_pred))\n",
    "    print(confusion_matrix(testLabel,gnb_pred))\n",
    "\n",
    "print(\"q = 10:\")\n",
    "pca_library(10)\n",
    "print(\"q = 50:\")\n",
    "pca_library(50)\n",
    "print(\"q = 100:\")\n",
    "pca_library(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run logistic regression on bag of words performs the best in the prediction task, which has 0.8216\n",
    "accuracy rate, because naive bayes will be better only when features are independent, but\n",
    "also when dependencies of features from each other are similar between features, but this is\n",
    "not the case in the context of reviews. 2-gram is worse because the combination of the adjacent\n",
    "words provides less additional information on classifying the comment sentiment but strip more\n",
    "relevant information provided by single word that is critical. That explains the reason that Logistic\n",
    "regression and naive bayes are worse off after applying 2-gram. For PCA for bag of words, it reduces\n",
    "dimensionality by performing SVD on rank q approximation, at the mean time, it reduces the\n",
    "computation but also throw away less relevant features, therefore, it is reasonable to get less\n",
    "accurate results. However, they are not off by a lot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
